import os
import re
import numpy as np
import openai
import torch
import open_clip
import traceback
from PIL import Image
from dotenv import load_dotenv
from pinecone import Pinecone
from pathlib import Path

def validate_environment():
    """ç’°å¢ƒå¤‰æ•°ã®æ¤œè¨¼"""
    load_dotenv()
    required_vars = {
        "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY"),
        "PINECONE_API_KEY": os.getenv("PINECONE_API_KEY")
    }
    missing_vars = [var for var, value in required_vars.items() if not value]
    if missing_vars:
        raise EnvironmentError(f"å¿…è¦ãªç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“: {', '.join(missing_vars)}")
    return required_vars

def initialize_services():
    """ã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–"""
    env_vars = validate_environment()
    client = openai.OpenAI(api_key=env_vars["OPENAI_API_KEY"])
    pc = Pinecone(api_key=env_vars["PINECONE_API_KEY"])
    index = pc.Index("text-search")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    returned_values = open_clip.create_model_and_transforms(
        "ViT-B/32",
        pretrained="openai",
        jit=True,
        force_quick_gelu=True
    )
    
    if len(returned_values) == 2:
        model, preprocess = returned_values
    elif len(returned_values) == 3:
        model, preprocess, _ = returned_values
    else:
        raise ValueError("open_clip.create_model_and_transforms ã‹ã‚‰ã®äºˆæœŸã—ãªã„æˆ»ã‚Šå€¤ã®æ•°ã§ã™")
    
    model.to(device)
    return client, index, model, preprocess, device

def extract_and_merge_fig_texts(file_path):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å…¨ã¦ã®Figureèª¬æ˜ã‚’æŠ½å‡ºã—ã€é–¢é€£ã™ã‚‹èª¬æ˜ã‚’çµ±åˆã™ã‚‹"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            text_data = f.read()
        
        # å…¨ã¦ã®Figureèª¬æ˜ã‚’æŠ½å‡º
        pattern = r"\[Fig(\d+[a-z]?)\]\s*(.*?)(?=\n\[Fig|$)"
        matches = re.findall(pattern, text_data, re.DOTALL)
        
        # Figureã”ã¨ã«èª¬æ˜æ–‡ã‚’æ•´ç†
        fig_groups = {}
        for num, desc in matches:
            # ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹Figureç•ªå·ã‚’æŠ½å‡ºï¼ˆä¾‹ï¼šFig1a â†’ Fig1ï¼‰
            base_fig = re.match(r'(\d+)', num).group(1)
            base_key = f"Fig{base_fig}"
            
            # èª¬æ˜æ–‡ã‚’æ•´å½¢
            cleaned_desc = desc.strip()
            
            # ãƒ™ãƒ¼ã‚¹Figureã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            if base_key not in fig_groups:
                fig_groups[base_key] = {
                    'main_text': [],
                    'sub_figs': {},
                    'full_id': f"Fig{num}"
                }
            
            # ã‚µãƒ–ãƒ•ã‚£ã‚®ãƒ¥ã‚¢ãŒã‚ã‚‹å ´åˆï¼ˆFig1a, Fig1b ãªã©ï¼‰
            if len(num) > len(base_fig):
                sub_key = f"Fig{num}"
                fig_groups[base_key]['sub_figs'][sub_key] = cleaned_desc
            else:
                fig_groups[base_key]['main_text'].append(cleaned_desc)

        # æœ€çµ‚çš„ãªèª¬æ˜æ–‡ã‚’æ§‹ç¯‰
        final_texts = {}
        for base_key, group in fig_groups.items():
            combined_text = []
            
            # ãƒ¡ã‚¤ãƒ³ã®èª¬æ˜æ–‡ã‚’è¿½åŠ 
            if group['main_text']:
                combined_text.extend(group['main_text'])
            
            # ã‚µãƒ–ãƒ•ã‚£ã‚®ãƒ¥ã‚¢ã®èª¬æ˜æ–‡ã‚’è¿½åŠ 
            if group['sub_figs']:
                sub_text = [f"{sub_id}: {desc}" for sub_id, desc in sorted(group['sub_figs'].items())]
                combined_text.extend(sub_text)
            
            # æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
            final_texts[base_key] = " ".join(combined_text)

        print(f"\nğŸ“š {len(final_texts)}å€‹ã®Figureèª¬æ˜ã‚’æŠ½å‡ºãƒ»çµ±åˆã—ã¾ã—ãŸ")
        for fig_id, text in final_texts.items():
            print(f"\nâœ… {fig_id}ã®èª¬æ˜æ–‡ã‚’çµ±åˆ:")
            print(f"   é•·ã•: {len(text)}æ–‡å­—")
            print(f"   ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {text[:100]}...")
        
        return final_texts

    except Exception as e:
        print(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        print(traceback.format_exc())
        return {}

def check_image_files(fig_texts):
    """ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ã‚’ç¢ºèª"""
    data_dir = Path("data")
    missing_images = []
    existing_images = []
    
    for fig_id in fig_texts.keys():
        image_path = data_dir / f"{fig_id}.jpg"
        if not image_path.exists():
            missing_images.append(fig_id)
        else:
            existing_images.append(fig_id)
    
    if missing_images:
        print("\nâš ï¸ ä»¥ä¸‹ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“:")
        for fig_id in missing_images:
            print(f"  - data/{fig_id}.jpg")
    
    if existing_images:
        print("\nâœ… ä»¥ä¸‹ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã§ã™:")
        for fig_id in existing_images:
            print(f"  - data/{fig_id}.jpg")
    
    return existing_images, missing_images

def get_image_embedding(image_path, model, preprocess, device):
    """ç”»åƒã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’å–å¾—"""
    try:
        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
        with torch.no_grad():
            image_embedding = model.encode_image(image).cpu().numpy().flatten()
        return image_embedding
    except Exception as e:
        print(f"ç”»åƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ ({image_path}): {e}")
        return np.zeros(512)

def get_text_embedding(text, client):
    """ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’å–å¾—"""
    try:
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        return np.zeros(1536)

def main():
    try:
        # ã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–
        client, index, model, preprocess, device = initialize_services()
        
        file_path = "data/chapter2_test.txt"
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
        
        # æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‰Šé™¤
        index.delete(delete_all=True)
        print("ğŸ—‘ï¸ æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        
        # Figureèª¬æ˜ã®æŠ½å‡ºã¨çµ±åˆ
        fig_texts = extract_and_merge_fig_texts(file_path)
        
        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
        existing_images, missing_images = check_image_files(fig_texts)

        # Pineconeã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        to_upsert = []
        for fig_id, text in fig_texts.items():
            # ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹Figure IDã‚’ä½¿ç”¨
            base_fig_id = fig_id
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°
            text_emb = get_text_embedding(text, client)
            
            # ç”»åƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
            combined_emb = text_emb
            image_paths = []
            
            # ãƒ¡ã‚¤ãƒ³ã®ç”»åƒã‚’ãƒã‚§ãƒƒã‚¯
            main_image_path = f"data/{base_fig_id}.jpg"
            if os.path.exists(main_image_path):
                image_paths.append(main_image_path)
            
            # ã‚µãƒ–ãƒ•ã‚£ã‚®ãƒ¥ã‚¢ã®ç”»åƒã‚’ãƒã‚§ãƒƒã‚¯
            if any(c.isdigit() for c in base_fig_id):
                base_num = ''.join(filter(str.isdigit, base_fig_id))
                for suffix in ['a', 'b', 'c', 'd']:
                    sub_image_path = f"data/{base_fig_id}{suffix}.jpg"
                    if os.path.exists(sub_image_path):
                        image_paths.append(sub_image_path)
            
            # ç”»åƒãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’çµåˆ
            if image_paths:
                image_embeddings = []
                for img_path in image_paths:
                    img_emb = get_image_embedding(img_path, model, preprocess, device)
                    img_emb = np.tile(img_emb, 1536 // len(img_emb))
                    image_embeddings.append(img_emb)
                
                if image_embeddings:
                    # å…¨ã¦ã®ç”»åƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã®å¹³å‡ã‚’å–ã‚‹
                    avg_image_emb = np.mean(image_embeddings, axis=0)
                    # ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’çµåˆ
                    combined_emb = np.mean([text_emb, avg_image_emb], axis=0)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
            metadata = {
                "figure_id": base_fig_id,
                "text": text
            }
            if image_paths:
                metadata["image_paths"] = image_paths
            
            # ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã§ãªã„ã“ã¨ã‚’ç¢ºèª
            if not np.allclose(combined_emb, np.zeros_like(combined_emb)):
                to_upsert.append((base_fig_id, combined_emb.tolist(), metadata))

        # Pineconeã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        if to_upsert:
            index.upsert(vectors=to_upsert)
            print(f"\nâœ¨ {len(to_upsert)}å€‹ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’Pineconeã«ä¿å­˜ã—ã¾ã—ãŸ")
            
            # ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦ã‚’è¡¨ç¤º
            print("\nğŸ“Š ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦:")
            for vector_id, _, metadata in to_upsert:
                print(f"\nğŸ”¹ {vector_id}:")
                print(f"   ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(metadata['text'])}æ–‡å­—")
                if 'image_paths' in metadata:
                    print(f"   é–¢é€£ç”»åƒ: {len(metadata['image_paths'])}æš")
        else:
            print("\nâš ï¸ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print(traceback.format_exc())

if __name__ == "__main__":
    main()